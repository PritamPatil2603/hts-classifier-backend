const OpenAI = require('openai');
const config = require('./config/config');

// Initialize OpenAI client (same as your service)
const openai = new OpenAI({
  apiKey: config.openai.apiKey,
  maxRetries: 2
});

async function testBasicOpenAIResponse() {
  console.log('\nüß™ TESTING BASIC OPENAI RESPONSE API');
  console.log('=' .repeat(50));
  
  const productDescription = "Laptop computer with Intel processor, 16GB RAM, 512GB SSD, Windows 11";
  
  const input = [
    {
      role: "system",
      content: config.systemPrompt
    },
    {
      role: "user", 
      content: productDescription
    }
  ];

  console.log(`üìù Product: ${productDescription}`);
  console.log(`ü§ñ Model: ${config.openai.model}`);
  console.log(`üìè System prompt length: ${config.systemPrompt.length} chars`);
  
  const startTime = Date.now();
  
  try {
    // Test 1: Minimal configuration (no tools, no optimizations)
    console.log('\nüî¨ TEST 1: Minimal Configuration');
    const minimalConfig = {
      model: config.openai.model,
      input: input,
      store: true
    };
    
    console.log('‚è±Ô∏è Starting minimal API call...');
    const minimalStart = Date.now();
    const minimalResponse = await openai.responses.create(minimalConfig);
    const minimalDuration = Date.now() - minimalStart;
    
    console.log(`‚úÖ Minimal response: ${minimalResponse.id}`);
    console.log(`‚è±Ô∏è Minimal duration: ${minimalDuration}ms`);
    console.log(`üìä Output items: ${minimalResponse.output?.length || 0}`);
    
    // Test 2: With your optimizations (but no tools)
    console.log('\nüî¨ TEST 2: With Optimizations (No Tools)');
    const optimizedConfig = {
      model: config.openai.model,
      input: input,
      temperature: 0.00,
      max_output_tokens: 1500,
      top_p: 0.85,
      parallel_tool_calls: true,
      store: true
    };
    
    console.log('‚è±Ô∏è Starting optimized API call...');
    const optimizedStart = Date.now();
    const optimizedResponse = await openai.responses.create(optimizedConfig);
    const optimizedDuration = Date.now() - optimizedStart;
    
    console.log(`‚úÖ Optimized response: ${optimizedResponse.id}`);
    console.log(`‚è±Ô∏è Optimized duration: ${optimizedDuration}ms`);
    console.log(`üìä Output items: ${optimizedResponse.output?.length || 0}`);
    
    // Test 3: Chat completion for comparison
    console.log('\nüî¨ TEST 3: Chat Completion (Baseline)');
    const chatConfig = {
      model: config.openai.model,
      messages: [
        { role: "system", content: config.systemPrompt },
        { role: "user", content: productDescription }
      ],
      temperature: 0.00,
      max_tokens: 1500,
      top_p: 0.85
    };
    
    console.log('‚è±Ô∏è Starting chat completion...');
    const chatStart = Date.now();
    const chatResponse = await openai.chat.completions.create(chatConfig);
    const chatDuration = Date.now() - chatStart;
    
    console.log(`‚úÖ Chat completion: ${chatResponse.id}`);
    console.log(`‚è±Ô∏è Chat duration: ${chatDuration}ms`);
    console.log(`üìä Response length: ${chatResponse.choices[0]?.message?.content?.length || 0} chars`);
    
    // Summary
    const totalDuration = Date.now() - startTime;
    console.log('\nüìä PERFORMANCE SUMMARY');
    console.log('=' .repeat(50));
    console.log(`üîπ Minimal Responses API: ${minimalDuration}ms`);
    console.log(`üîπ Optimized Responses API: ${optimizedDuration}ms`);
    console.log(`üîπ Chat Completions API: ${chatDuration}ms`);
    console.log(`üîπ Total test time: ${totalDuration}ms`);
    
    // Analysis
    console.log('\nüîç ANALYSIS');
    console.log('=' .repeat(50));
    if (minimalDuration > 8000) {
      console.log(`üö® SLOW: Minimal Responses API took ${minimalDuration}ms - this suggests model/prompt issues`);
    }
    if (optimizedDuration > minimalDuration + 1000) {
      console.log(`‚ö†Ô∏è WARNING: Optimizations added ${optimizedDuration - minimalDuration}ms overhead`);
    }
    if (chatDuration < minimalDuration) {
      console.log(`üí° INSIGHT: Chat API is ${minimalDuration - chatDuration}ms faster than Responses API`);
    }
    
    return {
      minimal: { duration: minimalDuration, responseId: minimalResponse.id },
      optimized: { duration: optimizedDuration, responseId: optimizedResponse.id },
      chat: { duration: chatDuration, responseId: chatResponse.id }
    };
    
  } catch (error) {
    const errorDuration = Date.now() - startTime;
    console.error(`‚ùå Error after ${errorDuration}ms:`, error.message);
    console.error(`üîç Error type: ${error.constructor.name}`);
    console.error(`üîç Error code: ${error.code || 'N/A'}`);
    console.error(`üîç Error status: ${error.status || 'N/A'}`);
    throw error;
  }
}

// Run the test
if (require.main === module) {
  console.log('üöÄ Starting OpenAI Performance Test...');
  testBasicOpenAIResponse()
    .then(results => {
      console.log('\n‚úÖ Test completed successfully!');
      process.exit(0);
    })
    .catch(error => {
      console.error('\n‚ùå Test failed:', error.message);
      process.exit(1);
    });
}

module.exports = { testBasicOpenAIResponse };